{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "large scale data processing \n",
    "use Python, Scala, or Java to process data\n",
    "\n",
    "scalable, fast, directed acyclic graph to optimize workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLLib - machine learning\n",
    "Spark SQL\n",
    "GraphX\n",
    "Spark Streaming "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resilent Distributed Dataset\n",
    "handle failure\n",
    "\n",
    "store valued inforamtion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark code in Python:\n",
    "find the worst rated movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark-submit ...py (in terminal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "def loadMovieNames():\n",
    "    '''\n",
    "    open the file and convert each line into a dictionary object with {movie id: moviename}\n",
    "    '''\n",
    "    movieNames = {}\n",
    "    with open(\"ml-100k/u.iten\") as f:\n",
    "        for line in f:\n",
    "            fields = line.split('|')\n",
    "            movieNames[int(field[0])] = fields[1]\n",
    "    return movieNames\n",
    "\n",
    "def parseInput(line):\n",
    "    '''\n",
    "    convert each line into a  object with (movie id,(rating, 1.0))\n",
    "    '''\n",
    "    fields=line.split()\n",
    "    return (int(fields[1]),(float(fields[2]),1.0))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #create spark context\n",
    "    conf = SparkConf().setAppName(\"worstmovies\")\n",
    "    sc = SparkContext(conf = conf)\n",
    "    \n",
    "    movieNames = loadMovieNames()\n",
    "    \n",
    "    #load up the raw u.data file\n",
    "    lines = sc.textfile(\"...\")\n",
    "    \n",
    "    movieRatings = lines.map(parseInput)\n",
    "    \n",
    "    #sum of ratings, totoal ratings\n",
    "    ratingTotalsAndCount = movieRatings.reduceByKey(lambda movie1, movie2: (movie1[0] + movie2[0], movie1[1] + movie2[1] ) )\n",
    "    \n",
    "    averageRatings = ratingTotalsAndCount.mapValues(lambda totalAndCount:totalAndCount[0] / totalAndCount[1])\n",
    "    \n",
    "    sortedMovies = averageRatings.sortBy(lambda x:x[1])\n",
    "    \n",
    "    results = sortedMovies.take(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find the worst rated movie that is rated by at least 10 people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "def loadMovieNames():\n",
    "    '''\n",
    "    open the file and convert each line into a dictionary object with {movie id: moviename}\n",
    "    '''\n",
    "    movieNames = {}\n",
    "    with open(\"ml-100k/u.iten\") as f:\n",
    "        for line in f:\n",
    "            fields = line.split('|')\n",
    "            movieNames[int(field[0])] = fields[1]\n",
    "    return movieNames\n",
    "\n",
    "def parseInput(line):\n",
    "    '''\n",
    "    convert each line into a  object with (movie id,(rating, 1.0))\n",
    "    '''\n",
    "    fields=line.split()\n",
    "    return (int(fields[1]),(float(fields[2]),1.0))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #create spark context\n",
    "    conf = SparkConf().setAppName(\"worstmovies\")\n",
    "    sc = SparkContext(conf = conf)\n",
    "    \n",
    "    movieNames = loadMovieNames()\n",
    "    \n",
    "    #load up the raw u.data file\n",
    "    lines = sc.textfile(\"...\")\n",
    "    \n",
    "    movieRatings = lines.map(parseInput)\n",
    "    \n",
    "    ratingTotalsAndCount = movieRatings.reduceByKrk(lambda movie1, movie2: (movie1[0] + movie2[0], movie1[1] + movie2[1] ) )\n",
    "    \n",
    "    #(movie id,(rating, 1.0)) x[1]=(sum of rating, count)\n",
    "    populartotalsandcount = ratingTotalsAndCount.filter(lambda x:[1][1] > 10)\n",
    "    \n",
    "    averageRatings = populartotalsandcount.mapValues(lambda totalAndCcount:totalAndCount[0] / totalAndCount[1])\n",
    "    \n",
    "    sortedMovies = averageRatings.sortBy(lambda x:x[1])\n",
    "    \n",
    "    results = sortedMovies.take(10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkSQL in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext, Row\n",
    "\n",
    "hiveContext=HiveContext(sc)\n",
    "inputData=spark.read.json(dataFile)\n",
    "inputData.createOrReplaceTempView(\"myStructuredStuffed\")\n",
    "#create a database table \n",
    "myResultDataFrame=hiveContextsql('''\n",
    "                                SELECT foo FROM bar ORDER BY foobar\n",
    "                                ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myResultDataFrame.show()\n",
    "select filter groupby rdd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions\n",
    "\n",
    "def loadMovieNames():\n",
    "\tmovieNames = {}\n",
    "\twith open('ml-100k/u.item') as f:\n",
    "\t\tfor line in f:\n",
    "\t\t\tfields = line.split('|')\n",
    "\t\t\tmovieNames[int(fields[0])] = fields[1]\n",
    "\treturn movieNames\n",
    "\n",
    "def parseInput(line):\n",
    "\tfields = line.split()\n",
    "\treturn Row(movieID = int(fields[1]), rating = float(fields[2]))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\t# Create a SparkSession\n",
    "\tspark = SparkSession.builder.appName(\"PopularMovies\").getOrCreate()\n",
    "\n",
    "\t# load up our movie ID -> name dictionary\n",
    "\tmovieNames = loadMovieNames()\n",
    "\n",
    "\t# get the raw data\n",
    "\tlines = spark.sparkContext.textFile(\"hdfs:///user/maria_dev/ml-100k/u.data\")\n",
    "\n",
    "\t# Convert it to a RDD of Row objects with (movieID, rating)\n",
    "\tmovies = lines.map(parseInput)\n",
    "\n",
    "\t# Convert that to a DataFrame\n",
    "\tmovieDataset = spark.createDataFrame(movies)\n",
    "\n",
    "\t# Compute counts of ratings for each movieID\n",
    "\tcounts = movieDataset.groupBy(\"movieID\").count()\n",
    "\n",
    "\t# Compute average rating for each movieID\n",
    "\taverageRatings = movieDataset.groupBy(\"movieID\").avg(\"rating\")\n",
    "\n",
    "\t# Join the two together (We now have movieID, avg(rating), and count columns)\n",
    "\taveragesAndCounts = counts.join(averageRatings, \"movieID\")\n",
    "\n",
    "\t# Pull the top 10 results\n",
    "\ttopTen = averagesAndCounts.orderBy(\"avg(rating)\").take(10)\n",
    "\n",
    "    # Print them out, converting movie ID's to names as we go.\n",
    "    for movie in topTen:\n",
    "   \t\tprint (movieNames[movie[0]], movie[1], movie[2])\n",
    "\n",
    "    # Stop the session\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find the worst rated movie that is rated by at least 10 people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions\n",
    "\n",
    "def loadMovieNames():\n",
    "\tmovieNames = {}\n",
    "\twith open('ml-100k/u.item') as f:\n",
    "\t\tfor line in f:\n",
    "\t\t\tfields = line.split('|')\n",
    "\t\t\tmovieNames[int(fields[0])] = fields[1]\n",
    "\treturn movieNames\n",
    "\n",
    "def parseInput(line):\n",
    "\tfields = line.split()\n",
    "\treturn Row(movieID = int(fields[1]), rating = float(fields[2]))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\t# Create a SparkSession\n",
    "\tspark = SparkSession.builder.appName(\"PopularMovies\").getOrCreate()\n",
    "\n",
    "\t# load up our movie ID -> name dictionary\n",
    "\tmovieNames = loadMovieNames()\n",
    "\n",
    "\t# get the raw data\n",
    "\tlines = spark.sparkContext.textFile(\"hdfs:///user/maria_dev/ml-100k/u.data\")\n",
    "\n",
    "\t# Convert it to a RDD of Row objects with (movieID, rating)\n",
    "\tmovies = lines.map(parseInput)\n",
    "\n",
    "\t# Convert that to a DataFrame\n",
    "\tmovieDataset = spark.createDataFrame(movies)\n",
    "\n",
    "\t# Compute counts of ratings for each movieID\n",
    "\tcounts = movieDataset.groupBy(\"movieID\").count()\n",
    "\n",
    "\t# Filter the count of ratings for movies < 10 or fewer times\n",
    "\tfilteredCounts = counts.filter(\"count > 10\")\n",
    "\n",
    "\t# Compute average rating for each movieID\n",
    "\taverageRatings = movieDataset.groupBy(\"movieID\").avg(\"rating\")\n",
    "\n",
    "\t# Join the two together (We now have movieID, avg(rating), and count columns)\n",
    "\taveragesAndCounts = filteredCounts.join(averageRatings, \"movieID\")\n",
    "\n",
    "\t# Pull the top 10 results\n",
    "\ttopTen = averagesAndCounts.orderBy(\"avg(rating)\").take(10)\n",
    "\n",
    "    # Print them out, converting movie ID's to names as we go.\n",
    "    for movie in topTen:\n",
    "   \t\tprint (movieNames[movie[0]], movie[1], movie[2])\n",
    "\n",
    "    # Stop the session\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use ML-ALS to recommend movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Load up movie ID -> movie name dictionary\n",
    "def loadMovieNames():\n",
    "    movieNames = {}\n",
    "    with open(\"ml-100k/u.item\") as f:\n",
    "        for line in f:\n",
    "            fields = line.split('|')\n",
    "            movieNames[int(fields[0])] = fields[1].decode('ascii', 'ignore')\n",
    "    return movieNames\n",
    "\n",
    "# Convert u.data lines into (userID, movieID, rating) rows\n",
    "def parseInput(line):\n",
    "    fields = line.value.split()\n",
    "    return Row(userID = int(fields[0]), movieID = int(fields[1]), rating = float(fields[2]))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a SparkSession (the config bit is only for Windows!)\n",
    "    spark = SparkSession.builder.appName(\"MovieRecs\").getOrCreate()\n",
    "\n",
    "    # Load up our movie ID -> name dictionary\n",
    "    movieNames = loadMovieNames()\n",
    "\n",
    "    # Get the raw data \n",
    "    lines = spark.read.text(\"hdfs:///user/maria_dev/ml-100k/u.data\").rdd\n",
    "\n",
    "    # Convert it to a RDD of Row objects with (userID, movieID, rating)\n",
    "    ratingsRDD = lines.map(parseInput)\n",
    "\n",
    "    # Convert to a DataFrame and cache it\n",
    "    ratings = spark.createDataFrame(ratingsRDD).cache()\n",
    "\n",
    "    # Create an ALS collaborative filtering model from the complete data set\n",
    "    als = ALS(maxIter=5, regParam=0.01, userCol=\"userID\", itemCol=\"movieID\", ratingCol=\"rating\")\n",
    "    model = als.fit(ratings)\n",
    "\n",
    "    # Print out ratings from user 0:\n",
    "    print(\"\\nRatings for user ID 0:\")\n",
    "    userRatings = ratings.filter(\"userID = 0\")\n",
    "    for rating in userRatings.collect():\n",
    "        print movieNames[rating['movieID']], rating['rating']\n",
    "\n",
    "    print(\"\\nTop 20 recommendations:\")\n",
    "    # Find movies rated more than 100 times\n",
    "    ratingCounts = ratings.groupBy(\"movieID\").count().filter(\"count > 100\")\n",
    "    # Construct a \"test\" dataframe for user 0 with every movie rated more than 100 times\n",
    "    popularMovies = ratingCounts.select(\"movieID\").withColumn('userID', lit(0))\n",
    "\n",
    "    # Run our model on that list of popular movies for user ID 0\n",
    "    recommendations = model.transform(popularMovies)\n",
    "\n",
    "    # Get the top 20 movies with the highest predicted rating for this user\n",
    "    topRecommendations = recommendations.sort(recommendations.prediction.desc()).take(20)\n",
    "\n",
    "    for recommendation in topRecommendations:\n",
    "        print (movieNames[recommendation['movieID']], recommendation['prediction'])\n",
    "\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
